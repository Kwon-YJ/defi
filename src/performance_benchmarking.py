"""
Performance Benchmarking Module for DEFIPOSER-ARB
ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë° ë²¤ì¹˜ë§ˆí‚¹ êµ¬í˜„ (ë…¼ë¬¸ 6.43ì´ˆ ëª©í‘œ)

ì´ ëª¨ë“ˆì€ ë…¼ë¬¸ì˜ ì„±ëŠ¥ ê¸°ì¤€ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë° ë²¤ì¹˜ë§ˆí‚¹ì„ ì œê³µí•©ë‹ˆë‹¤.
- ëª©í‘œ: í‰ê·  6.43ì´ˆ ì´í•˜ ì‹¤í–‰ ì‹œê°„
- ë¸”ë¡ë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì 
- ì„±ëŠ¥ ë³‘ëª©ì  ì‹ë³„
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import time
import asyncio
import statistics
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from contextlib import contextmanager
import psutil
import threading
from collections import deque, defaultdict
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    block_number: int
    total_execution_time: float  # seconds
    graph_building_time: float
    negative_cycle_detection_time: float
    local_search_time: float
    parameter_optimization_time: float
    validation_time: float
    memory_usage: float  # MB
    cpu_usage: float  # percentage
    opportunities_found: int
    strategies_executed: int
    total_revenue: float
    gas_cost: float

@dataclass
class ComponentTiming:
    """ì»´í¬ë„ŒíŠ¸ë³„ íƒ€ì´ë° ì •ë³´"""
    name: str
    start_time: float
    end_time: float
    duration: float
    cpu_before: float
    cpu_after: float
    memory_before: float
    memory_after: float

class PerformanceBenchmarker:
    """
    DEFIPOSER-ARB ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í´ë˜ìŠ¤
    ë…¼ë¬¸ì˜ 6.43ì´ˆ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ë„êµ¬
    """
    
    def __init__(self, target_time: float = 6.43, history_size: int = 1000):
        """
        Args:
            target_time: ëª©í‘œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            history_size: ì„±ëŠ¥ ê¸°ë¡ ë³´ê´€ ê°œìˆ˜
        """
        self.target_time = target_time
        self.history_size = history_size
        
        # ì„±ëŠ¥ ê¸°ë¡ ì €ì¥
        self.performance_history = deque(maxlen=history_size)
        self.component_timings = defaultdict(list)
        
        # í˜„ì¬ ì¸¡ì • ì¤‘ì¸ ë©”íŠ¸ë¦­
        self.current_metrics: Optional[PerformanceMetrics] = None
        self.timing_stack: List[ComponentTiming] = []
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_blocks_processed': 0,
            'blocks_under_target': 0,
            'blocks_over_target': 0,
            'average_execution_time': 0.0,
            'fastest_execution': float('inf'),
            'slowest_execution': 0.0,
            'success_rate': 0.0
        }
        
        # ê²½ê³  ì„ê³„ê°’
        self.warning_threshold = target_time * 0.8  # 80% of target time
        self.critical_threshold = target_time  # 100% of target time
        
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        self.log_file = Path("logs/performance_benchmark.json")
        self.log_file.parent.mkdir(exist_ok=True)
        
    def start_block_processing(self, block_number: int) -> None:
        """ë¸”ë¡ ì²˜ë¦¬ ì‹œì‘"""
        self.current_metrics = PerformanceMetrics(
            timestamp=datetime.now(),
            block_number=block_number,
            total_execution_time=0.0,
            graph_building_time=0.0,
            negative_cycle_detection_time=0.0,
            local_search_time=0.0,
            parameter_optimization_time=0.0,
            validation_time=0.0,
            memory_usage=self._get_memory_usage(),
            cpu_usage=self._get_cpu_usage(),
            opportunities_found=0,
            strategies_executed=0,
            total_revenue=0.0,
            gas_cost=0.0
        )
        
        logger.info(f"ğŸš€ ë¸”ë¡ {block_number} ì²˜ë¦¬ ì‹œì‘ (ëª©í‘œ: {self.target_time}ì´ˆ)")
        
    def end_block_processing(self, opportunities_found: int = 0, 
                           strategies_executed: int = 0, 
                           total_revenue: float = 0.0,
                           gas_cost: float = 0.0) -> PerformanceMetrics:
        """ë¸”ë¡ ì²˜ë¦¬ ì™„ë£Œ ë° ë©”íŠ¸ë¦­ ì €ì¥"""
        if not self.current_metrics:
            raise ValueError("start_block_processing()ì„ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤")
            
        # ìµœì¢… ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        end_time = datetime.now()
        self.current_metrics.total_execution_time = (
            end_time - self.current_metrics.timestamp
        ).total_seconds()
        
        self.current_metrics.opportunities_found = opportunities_found
        self.current_metrics.strategies_executed = strategies_executed
        self.current_metrics.total_revenue = total_revenue
        self.current_metrics.gas_cost = gas_cost
        
        # ì„±ëŠ¥ ê¸°ë¡ ì €ì¥
        self.performance_history.append(self.current_metrics)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats()
        
        # ì„±ëŠ¥ í‰ê°€ ë° ë¡œê¹…
        self._evaluate_performance(self.current_metrics)
        
        # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
        self._save_to_log()
        
        result = self.current_metrics
        self.current_metrics = None
        
        return result
    
    @contextmanager
    def time_component(self, component_name: str):
        """ì»´í¬ë„ŒíŠ¸ë³„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        timing = ComponentTiming(
            name=component_name,
            start_time=time.perf_counter(),
            end_time=0.0,
            duration=0.0,
            cpu_before=self._get_cpu_usage(),
            cpu_after=0.0,
            memory_before=self._get_memory_usage(),
            memory_after=0.0
        )
        
        self.timing_stack.append(timing)
        
        try:
            yield timing
        finally:
            timing.end_time = time.perf_counter()
            timing.duration = timing.end_time - timing.start_time
            timing.cpu_after = self._get_cpu_usage()
            timing.memory_after = self._get_memory_usage()
            
            # í˜„ì¬ ë©”íŠ¸ë¦­ì— ì»´í¬ë„ŒíŠ¸ ì‹œê°„ ë°˜ì˜
            if self.current_metrics:
                self._update_component_timing(component_name, timing.duration)
                
            # ì»´í¬ë„ŒíŠ¸ íƒ€ì´ë° ê¸°ë¡ ì €ì¥
            self.component_timings[component_name].append(timing)
            
            # ìŠ¤íƒì—ì„œ ì œê±°
            if self.timing_stack and self.timing_stack[-1] == timing:
                self.timing_stack.pop()
                
            logger.debug(f"â±ï¸ {component_name}: {timing.duration:.3f}ì´ˆ")
    
    def _update_component_timing(self, component_name: str, duration: float) -> None:
        """ì»´í¬ë„ŒíŠ¸ë³„ íƒ€ì´ë°ì„ í˜„ì¬ ë©”íŠ¸ë¦­ì— ë°˜ì˜"""
        if not self.current_metrics:
            return
            
        if component_name == "graph_building":
            self.current_metrics.graph_building_time += duration
        elif component_name == "negative_cycle_detection":
            self.current_metrics.negative_cycle_detection_time += duration
        elif component_name == "local_search":
            self.current_metrics.local_search_time += duration
        elif component_name == "parameter_optimization":
            self.current_metrics.parameter_optimization_time += duration
        elif component_name == "validation":
            self.current_metrics.validation_time += duration
    
    def _get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def _get_cpu_usage(self) -> float:
        """í˜„ì¬ CPU ì‚¬ìš©ë¥  (%%)"""
        return psutil.cpu_percent()
    
    def _update_stats(self) -> None:
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        if not self.performance_history:
            return
            
        execution_times = [m.total_execution_time for m in self.performance_history]
        
        self.stats['total_blocks_processed'] = len(self.performance_history)
        self.stats['blocks_under_target'] = sum(
            1 for t in execution_times if t <= self.target_time
        )
        self.stats['blocks_over_target'] = (
            self.stats['total_blocks_processed'] - self.stats['blocks_under_target']
        )
        self.stats['average_execution_time'] = statistics.mean(execution_times)
        self.stats['fastest_execution'] = min(execution_times)
        self.stats['slowest_execution'] = max(execution_times)
        self.stats['success_rate'] = (
            self.stats['blocks_under_target'] / self.stats['total_blocks_processed']
        )
    
    def _evaluate_performance(self, metrics: PerformanceMetrics) -> None:
        """ì„±ëŠ¥ í‰ê°€ ë° ê²½ê³ """
        execution_time = metrics.total_execution_time
        
        if execution_time <= self.warning_threshold:
            logger.info(f"âœ… ë¸”ë¡ {metrics.block_number}: {execution_time:.3f}ì´ˆ "
                       f"(ëª©í‘œ {self.target_time}ì´ˆ ëŒ€ë¹„ ìš°ìˆ˜)")
        elif execution_time <= self.critical_threshold:
            logger.warning(f"âš ï¸ ë¸”ë¡ {metrics.block_number}: {execution_time:.3f}ì´ˆ "
                          f"(ëª©í‘œ {self.target_time}ì´ˆ ê·¼ì ‘)")
        else:
            logger.error(f"âŒ ë¸”ë¡ {metrics.block_number}: {execution_time:.3f}ì´ˆ "
                        f"(ëª©í‘œ {self.target_time}ì´ˆ ì´ˆê³¼)")
            
        # ì»´í¬ë„ŒíŠ¸ë³„ ë³‘ëª©ì  ë¶„ì„
        self._analyze_bottlenecks(metrics)
    
    def _analyze_bottlenecks(self, metrics: PerformanceMetrics) -> None:
        """ì„±ëŠ¥ ë³‘ëª©ì  ë¶„ì„"""
        components = {
            "ê·¸ë˜í”„ êµ¬ì¶•": metrics.graph_building_time,
            "ë„¤ê±°í‹°ë¸Œ ì‚¬ì´í´ íƒì§€": metrics.negative_cycle_detection_time,
            "ë¡œì»¬ ì„œì¹˜": metrics.local_search_time,
            "íŒŒë¼ë¯¸í„° ìµœì í™”": metrics.parameter_optimization_time,
            "ê²€ì¦": metrics.validation_time
        }
        
        # ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦° ì»´í¬ë„ŒíŠ¸ ì°¾ê¸°
        bottleneck = max(components, key=components.get)
        bottleneck_time = components[bottleneck]
        
        if bottleneck_time > self.target_time * 0.3:  # 30% ì´ìƒ
            logger.warning(f"ğŸ” ë³‘ëª©ì  ë°œê²¬: {bottleneck} ({bottleneck_time:.3f}ì´ˆ)")
    
    def _save_to_log(self) -> None:
        """ì„±ëŠ¥ ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥"""
        if not self.current_metrics:
            return
            
        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        metrics_dict = asdict(self.current_metrics)
        metrics_dict['timestamp'] = self.current_metrics.timestamp.isoformat()
        
        log_data = {
            "timestamp": self.current_metrics.timestamp.isoformat(),
            "metrics": metrics_dict,
            "stats": self.stats
        }
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_data, ensure_ascii=False, default=str) + '\n')
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_performance_report(self, last_n_blocks: Optional[int] = None) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        if not self.performance_history:
            return {"error": "ì„±ëŠ¥ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤"}
            
        # ìµœê·¼ Nê°œ ë¸”ë¡ë§Œ ë¶„ì„ (ì§€ì •ëœ ê²½ìš°)
        history = list(self.performance_history)
        if last_n_blocks:
            history = history[-last_n_blocks:]
            
        execution_times = [m.total_execution_time for m in history]
        
        report = {
            "summary": {
                "target_time": self.target_time,
                "blocks_analyzed": len(history),
                "success_rate": sum(1 for t in execution_times if t <= self.target_time) / len(history),
                "average_time": statistics.mean(execution_times),
                "median_time": statistics.median(execution_times),
                "fastest_time": min(execution_times),
                "slowest_time": max(execution_times),
                "std_deviation": statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            },
            "component_analysis": self._get_component_analysis(history),
            "resource_usage": self._get_resource_analysis(history),
            "recommendations": self._get_performance_recommendations(history)
        }
        
        return report
    
    def _get_component_analysis(self, history: List[PerformanceMetrics]) -> Dict[str, Any]:
        """ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ ë¶„ì„"""
        components = {
            "graph_building": [m.graph_building_time for m in history],
            "negative_cycle_detection": [m.negative_cycle_detection_time for m in history],
            "local_search": [m.local_search_time for m in history],
            "parameter_optimization": [m.parameter_optimization_time for m in history],
            "validation": [m.validation_time for m in history]
        }
        
        analysis = {}
        for name, times in components.items():
            if times and any(t > 0 for t in times):
                analysis[name] = {
                    "average": statistics.mean(times),
                    "max": max(times),
                    "percentage_of_total": (statistics.mean(times) / self.target_time) * 100
                }
        
        return analysis
    
    def _get_resource_analysis(self, history: List[PerformanceMetrics]) -> Dict[str, Any]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        memory_usage = [m.memory_usage for m in history if m.memory_usage > 0]
        cpu_usage = [m.cpu_usage for m in history if m.cpu_usage > 0]
        
        analysis = {}
        
        if memory_usage:
            analysis["memory"] = {
                "average_mb": statistics.mean(memory_usage),
                "max_mb": max(memory_usage),
                "min_mb": min(memory_usage)
            }
            
        if cpu_usage:
            analysis["cpu"] = {
                "average_percent": statistics.mean(cpu_usage),
                "max_percent": max(cpu_usage),
                "min_percent": min(cpu_usage)
            }
            
        return analysis
    
    def _get_performance_recommendations(self, history: List[PerformanceMetrics]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        execution_times = [m.total_execution_time for m in history]
        avg_time = statistics.mean(execution_times)
        
        if avg_time > self.target_time:
            recommendations.append(
                f"í‰ê·  ì‹¤í–‰ ì‹œê°„({avg_time:.3f}ì´ˆ)ì´ ëª©í‘œ({self.target_time}ì´ˆ)ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤."
            )
            
        # ì»´í¬ë„ŒíŠ¸ë³„ ë¶„ì„
        component_analysis = self._get_component_analysis(history)
        for name, data in component_analysis.items():
            if data["percentage_of_total"] > 40:  # 40% ì´ìƒ
                recommendations.append(
                    f"{name} ì»´í¬ë„ŒíŠ¸ê°€ ì „ì²´ ì‹œê°„ì˜ {data['percentage_of_total']:.1f}%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤. ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
                
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        resource_analysis = self._get_resource_analysis(history)
        if "memory" in resource_analysis:
            avg_memory = resource_analysis["memory"]["average_mb"]
            if avg_memory > 1000:  # 1GB ì´ìƒ
                recommendations.append(
                    f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰({avg_memory:.0f}MB)ì´ ë†’ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
                )
        
        if not recommendations:
            recommendations.append("ì„±ëŠ¥ì´ ëª©í‘œ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
            
        return recommendations
    
    def real_time_monitor(self, check_interval: int = 60) -> None:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        def monitor():
            while True:
                try:
                    if len(self.performance_history) >= 10:  # ìµœì†Œ 10ê°œ ê¸°ë¡ í•„ìš”
                        recent_report = self.get_performance_report(last_n_blocks=10)
                        
                        success_rate = recent_report["summary"]["success_rate"]
                        avg_time = recent_report["summary"]["average_time"]
                        
                        if success_rate < 0.8:  # 80% ë¯¸ë§Œ ì„±ê³µë¥ 
                            logger.warning(
                                f"ğŸš¨ ì„±ëŠ¥ ê²½ê³ : ìµœê·¼ ì„±ê³µë¥  {success_rate:.1%}, "
                                f"í‰ê·  ì‹¤í–‰ì‹œê°„ {avg_time:.3f}ì´ˆ"
                            )
                        else:
                            logger.info(
                                f"ğŸ“Š ì„±ëŠ¥ ì–‘í˜¸: ì„±ê³µë¥  {success_rate:.1%}, "
                                f"í‰ê·  ì‹¤í–‰ì‹œê°„ {avg_time:.3f}ì´ˆ"
                            )
                            
                    time.sleep(check_interval)
                    
                except Exception as e:
                    logger.error(f"ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                    time.sleep(check_interval)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        logger.info(f"ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì²´í¬ ê°„ê²©: {check_interval}ì´ˆ)")

# ì „ì—­ ë²¤ì¹˜ë§ˆì»¤ ì¸ìŠ¤í„´ìŠ¤
global_benchmarker = PerformanceBenchmarker()

# í¸ì˜ í•¨ìˆ˜ë“¤
def start_benchmarking(block_number: int) -> None:
    """ë¸”ë¡ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘"""
    global_benchmarker.start_block_processing(block_number)

def end_benchmarking(opportunities_found: int = 0, 
                    strategies_executed: int = 0,
                    total_revenue: float = 0.0,
                    gas_cost: float = 0.0) -> PerformanceMetrics:
    """ë¸”ë¡ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí‚¹ ì™„ë£Œ"""
    return global_benchmarker.end_block_processing(
        opportunities_found, strategies_executed, total_revenue, gas_cost
    )

def time_component(component_name: str):
    """ì»´í¬ë„ŒíŠ¸ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    return global_benchmarker.time_component(component_name)

def get_performance_report(last_n_blocks: Optional[int] = None) -> Dict[str, Any]:
    """ì„±ëŠ¥ ë³´ê³ ì„œ ì¡°íšŒ"""
    return global_benchmarker.get_performance_report(last_n_blocks)

def start_monitoring(check_interval: int = 60) -> None:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    global_benchmarker.real_time_monitor(check_interval)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import random
    
    print("ğŸ§ª Performance Benchmarker í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    start_monitoring(check_interval=10)
    
    # ê°€ìƒì˜ ë¸”ë¡ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
    for block_num in range(10000, 10010):
        start_benchmarking(block_num)
        
        # ê° ì»´í¬ë„ŒíŠ¸ ì‹œë®¬ë ˆì´ì…˜
        with time_component("graph_building"):
            time.sleep(random.uniform(0.5, 2.0))
            
        with time_component("negative_cycle_detection"):
            time.sleep(random.uniform(1.0, 3.0))
            
        with time_component("local_search"):
            time.sleep(random.uniform(0.5, 2.5))
            
        with time_component("parameter_optimization"):
            time.sleep(random.uniform(0.3, 1.5))
            
        with time_component("validation"):
            time.sleep(random.uniform(0.1, 0.5))
        
        # ë¸”ë¡ ì²˜ë¦¬ ì™„ë£Œ
        metrics = end_benchmarking(
            opportunities_found=random.randint(0, 5),
            strategies_executed=random.randint(0, 3),
            total_revenue=random.uniform(0, 10),
            gas_cost=random.uniform(0.01, 0.1)
        )
        
        print(f"ë¸”ë¡ {block_num}: {metrics.total_execution_time:.3f}ì´ˆ")
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ì¶œë ¥
    report = get_performance_report()
    print("\nğŸ“Š ì„±ëŠ¥ ë³´ê³ ì„œ:")
    print(f"ì„±ê³µë¥ : {report['summary']['success_rate']:.1%}")
    print(f"í‰ê·  ì‹¤í–‰ì‹œê°„: {report['summary']['average_time']:.3f}ì´ˆ")
    print(f"ìµœê³  ê¸°ë¡: {report['summary']['fastest_time']:.3f}ì´ˆ")
    
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for rec in report['recommendations']:
        print(f"- {rec}")