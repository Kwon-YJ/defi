#!/usr/bin/env python3
"""
DEFIPOSER-ARB Performance Analyzer
ì‹¤í–‰ ì‹œê°„ ë¶„ì„ ë° ìµœì í™” ê¶Œì¥ì‚¬í•­ ì œê³µ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì„±ëŠ¥ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ë³‘ëª©ì ì„ ì‹ë³„í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime, timedelta
import argparse
import numpy as np
from typing import Dict, List, Tuple
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.performance_benchmarking import get_performance_report
from src.logger import setup_logger

logger = setup_logger(__name__)

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self, log_file_path: str = "logs/performance_benchmark.json"):
        self.log_file_path = Path(log_file_path)
        self.target_time = 6.43  # ë…¼ë¬¸ ëª©í‘œ ì‹œê°„
        
        # í”Œë¡¯ ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
    
    def load_performance_data(self) -> pd.DataFrame:
        """ì„±ëŠ¥ ë¡œê·¸ ë°ì´í„° ë¡œë“œ"""
        if not self.log_file_path.exists():
            logger.error(f"ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.log_file_path}")
            return pd.DataFrame()
        
        data = []
        try:
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        entry = json.loads(line)
                        if 'metrics' in entry:
                            metrics = entry['metrics']
                            metrics['timestamp'] = pd.to_datetime(entry['timestamp'])
                            data.append(metrics)
            
            df = pd.DataFrame(data)
            logger.info(f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê¸°ë¡")
            return df
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def analyze_execution_times(self, df: pd.DataFrame) -> Dict:
        """ì‹¤í–‰ ì‹œê°„ ë¶„ì„"""
        if df.empty:
            return {"error": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        execution_times = df['total_execution_time']
        
        analysis = {
            "basic_stats": {
                "count": len(execution_times),
                "mean": execution_times.mean(),
                "median": execution_times.median(),
                "std": execution_times.std(),
                "min": execution_times.min(),
                "max": execution_times.max(),
                "target_achievement_rate": (execution_times <= self.target_time).mean()
            },
            "percentiles": {
                "p25": execution_times.quantile(0.25),
                "p50": execution_times.quantile(0.50),
                "p75": execution_times.quantile(0.75),
                "p90": execution_times.quantile(0.90),
                "p95": execution_times.quantile(0.95),
                "p99": execution_times.quantile(0.99)
            },
            "target_analysis": {
                "under_target": (execution_times <= self.target_time).sum(),
                "over_target": (execution_times > self.target_time).sum(),
                "worst_cases": execution_times.nlargest(5).tolist()
            }
        }
        
        return analysis
    
    def analyze_components(self, df: pd.DataFrame) -> Dict:
        """ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ ë¶„ì„"""
        if df.empty:
            return {"error": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        components = [
            'graph_building_time',
            'negative_cycle_detection_time', 
            'local_search_time',
            'parameter_optimization_time',
            'validation_time'
        ]
        
        component_analysis = {}
        
        for comp in components:
            if comp in df.columns:
                comp_data = df[comp]
                component_analysis[comp] = {
                    "mean": comp_data.mean(),
                    "median": comp_data.median(),
                    "max": comp_data.max(),
                    "std": comp_data.std(),
                    "percentage_of_target": (comp_data.mean() / self.target_time) * 100,
                    "bottleneck_frequency": (comp_data > self.target_time * 0.3).sum()  # 30% ì´ìƒì¸ ê²½ìš°
                }
        
        return component_analysis
    
    def identify_bottlenecks(self, df: pd.DataFrame) -> List[Dict]:
        """ì„±ëŠ¥ ë³‘ëª©ì  ì‹ë³„"""
        if df.empty:
            return []
        
        bottlenecks = []
        
        # ì»´í¬ë„ŒíŠ¸ë³„ ë³‘ëª©ì  ë¶„ì„
        components = [
            ('graph_building_time', 'ê·¸ë˜í”„ êµ¬ì¶•'),
            ('negative_cycle_detection_time', 'Negative Cycle íƒì§€'),
            ('local_search_time', 'Local Search'),
            ('parameter_optimization_time', 'íŒŒë¼ë¯¸í„° ìµœì í™”'),
            ('validation_time', 'ê²€ì¦')
        ]
        
        for comp_col, comp_name in components:
            if comp_col in df.columns:
                comp_data = df[comp_col]
                mean_time = comp_data.mean()
                
                if mean_time > self.target_time * 0.4:  # 40% ì´ìƒ
                    bottlenecks.append({
                        "component": comp_name,
                        "average_time": mean_time,
                        "percentage_of_target": (mean_time / self.target_time) * 100,
                        "severity": "ë†’ìŒ" if mean_time > self.target_time * 0.6 else "ì¤‘ê°„",
                        "recommendation": self._get_component_recommendation(comp_name)
                    })
        
        # ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ íŒ¨í„´ ë¶„ì„
        over_target_blocks = df[df['total_execution_time'] > self.target_time]
        if len(over_target_blocks) > 0:
            avg_over_time = over_target_blocks['total_execution_time'].mean()
            bottlenecks.append({
                "component": "ì „ì²´ ì‹œìŠ¤í…œ",
                "average_time": avg_over_time,
                "percentage_of_target": (avg_over_time / self.target_time) * 100,
                "severity": "ë†’ìŒ" if len(over_target_blocks) > len(df) * 0.3 else "ì¤‘ê°„",
                "frequency": len(over_target_blocks),
                "recommendation": "ì „ì²´ ì‹œìŠ¤í…œ ìµœì í™” í•„ìš”"
            })
        
        return sorted(bottlenecks, key=lambda x: x['average_time'], reverse=True)
    
    def _get_component_recommendation(self, component_name: str) -> str:
        """ì»´í¬ë„ŒíŠ¸ë³„ ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        recommendations = {
            "ê·¸ë˜í”„ êµ¬ì¶•": "ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê·¸ë˜í”„ í‘œí˜„, ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ë„ì…",
            "Negative Cycle íƒì§€": "Bellman-Ford ì•Œê³ ë¦¬ì¦˜ ìµœì í™”, ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ ",
            "Local Search": "Hill climbing ì•Œê³ ë¦¬ì¦˜ ìµœì í™”, ì‹œì‘ì  ì„ íƒ ê°œì„ ",
            "íŒŒë¼ë¯¸í„° ìµœì í™”": "ì´ì§„ íƒìƒ‰ ê°œì„ , ì´ˆê¸° ì¶”ì •ê°’ ì •í™•ë„ í–¥ìƒ",
            "ê²€ì¦": "ê²€ì¦ ë¡œì§ ê°„ì†Œí™”, ìºì‹± í™œìš©"
        }
        return recommendations.get(component_name, "ê°œë³„ ìµœì í™” ë°©ì•ˆ ê²€í†  í•„ìš”")
    
    def generate_performance_plots(self, df: pd.DataFrame, output_dir: str = "reports"):
        """ì„±ëŠ¥ ë¶„ì„ ê·¸ë˜í”„ ìƒì„±"""
        if df.empty:
            logger.error("ê·¸ë˜í”„ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # 1. ì‹¤í–‰ ì‹œê°„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.hist(df['total_execution_time'], bins=30, alpha=0.7, edgecolor='black')
        plt.axvline(self.target_time, color='red', linestyle='--', 
                   label=f'Target: {self.target_time}s')
        plt.xlabel('Execution Time (seconds)')
        plt.ylabel('Frequency')
        plt.title('Execution Time Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ì‹œê°„ë³„ ì‹¤í–‰ ì‹œê°„ ì¶”ì´
        plt.subplot(1, 2, 2)
        plt.plot(df['timestamp'], df['total_execution_time'], alpha=0.7, linewidth=1)
        plt.axhline(self.target_time, color='red', linestyle='--', 
                   label=f'Target: {self.target_time}s')
        plt.xlabel('Time')
        plt.ylabel('Execution Time (seconds)')
        plt.title('Execution Time Trend')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_path / 'execution_time_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ì»´í¬ë„ŒíŠ¸ë³„ ì‹œê°„ ë¶„ì„
        component_columns = [
            'graph_building_time',
            'negative_cycle_detection_time',
            'local_search_time', 
            'parameter_optimization_time',
            'validation_time'
        ]
        
        available_components = [col for col in component_columns if col in df.columns]
        
        if available_components:
            plt.figure(figsize=(12, 8))
            
            # ì»´í¬ë„ŒíŠ¸ë³„ ë°•ìŠ¤ í”Œë¡¯
            component_data = df[available_components]
            component_data.columns = [col.replace('_time', '').replace('_', ' ').title() 
                                    for col in component_data.columns]
            
            plt.subplot(2, 1, 1)
            component_data.boxplot(ax=plt.gca())
            plt.title('Component Execution Times (Box Plot)')
            plt.ylabel('Time (seconds)')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            
            # ì»´í¬ë„ŒíŠ¸ë³„ í‰ê·  ì‹œê°„ ë§‰ëŒ€ ê·¸ë˜í”„
            plt.subplot(2, 1, 2)
            means = component_data.mean()
            bars = plt.bar(means.index, means.values, alpha=0.7)
            plt.axhline(self.target_time * 0.3, color='orange', linestyle='--', 
                       label='30% of Target', alpha=0.7)
            plt.axhline(self.target_time * 0.5, color='red', linestyle='--', 
                       label='50% of Target', alpha=0.7)
            plt.title('Average Component Execution Times')
            plt.ylabel('Time (seconds)')
            plt.xticks(rotation=45)
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
            for i, bar in enumerate(bars):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.3f}s', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(output_path / 'component_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. ì„±ëŠ¥ ë‹¬ì„±ë¥  íŒŒì´ ì°¨íŠ¸
        plt.figure(figsize=(10, 5))
        
        plt.subplot(1, 2, 1)
        achievement_counts = [(df['total_execution_time'] <= self.target_time).sum(),
                             (df['total_execution_time'] > self.target_time).sum()]
        labels = ['Target Achieved', 'Target Missed']
        colors = ['green', 'red']
        plt.pie(achievement_counts, labels=labels, colors=colors, autopct='%1.1f%%', 
                startangle=90)
        plt.title('Target Achievement Rate')
        
        # 5. ê¸°íšŒ ë°œê²¬ ëŒ€ë¹„ ì‹¤í–‰ì‹œê°„ ì‚°ì ë„  
        plt.subplot(1, 2, 2)
        if 'opportunities_found' in df.columns:
            plt.scatter(df['opportunities_found'], df['total_execution_time'], 
                       alpha=0.6, s=30)
            plt.axhline(self.target_time, color='red', linestyle='--', alpha=0.7)
            plt.xlabel('Opportunities Found')
            plt.ylabel('Execution Time (seconds)')
            plt.title('Opportunities vs Execution Time')
            plt.grid(True, alpha=0.3)
        else:
            plt.text(0.5, 0.5, 'No opportunity data available', 
                    ha='center', va='center', transform=plt.gca().transAxes)
        
        plt.tight_layout()
        plt.savefig(output_path / 'performance_overview.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ì„±ëŠ¥ ë¶„ì„ ê·¸ë˜í”„ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def generate_report(self, output_file: str = "reports/performance_analysis_report.txt"):
        """ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        output_path = Path(output_file)
        output_path.parent.mkdir(exist_ok=True)
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_performance_data()
        
        if df.empty:
            logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ë¶„ì„ ìˆ˜í–‰
        execution_analysis = self.analyze_execution_times(df)
        component_analysis = self.analyze_components(df)
        bottlenecks = self.identify_bottlenecks(df)
        
        # ë³´ê³ ì„œ ì‘ì„±
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("DEFIPOSER-ARB ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ\n")
            f.write("="*60 + "\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ë¶„ì„ ëŒ€ìƒ: {len(df)}ê°œ ë¸”ë¡ ì²˜ë¦¬ ê¸°ë¡\n")
            f.write(f"ëª©í‘œ ì‹œê°„: {self.target_time}ì´ˆ\n\n")
            
            # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
            f.write("1. ì‹¤í–‰ ì‹œê°„ ë¶„ì„\n")
            f.write("-"*30 + "\n")
            stats = execution_analysis["basic_stats"]
            f.write(f"â€¢ í‰ê·  ì‹¤í–‰ ì‹œê°„: {stats['mean']:.3f}ì´ˆ\n")
            f.write(f"â€¢ ì¤‘ê°„ê°’: {stats['median']:.3f}ì´ˆ\n")
            f.write(f"â€¢ í‘œì¤€í¸ì°¨: {stats['std']:.3f}ì´ˆ\n")
            f.write(f"â€¢ ìµœì†Œ/ìµœëŒ€: {stats['min']:.3f}ì´ˆ / {stats['max']:.3f}ì´ˆ\n")
            f.write(f"â€¢ ëª©í‘œ ë‹¬ì„±ë¥ : {stats['target_achievement_rate']:.1%}\n\n")
            
            # ë°±ë¶„ìœ„ìˆ˜ ë¶„ì„
            f.write("ë°±ë¶„ìœ„ìˆ˜ ë¶„ì„:\n")
            percentiles = execution_analysis["percentiles"]
            for p, value in percentiles.items():
                f.write(f"â€¢ {p}: {value:.3f}ì´ˆ\n")
            f.write("\n")
            
            # ì»´í¬ë„ŒíŠ¸ ë¶„ì„
            if component_analysis and "error" not in component_analysis:
                f.write("2. ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ ë¶„ì„\n")
                f.write("-"*30 + "\n")
                
                for comp, data in component_analysis.items():
                    comp_name = comp.replace('_time', '').replace('_', ' ').title()
                    f.write(f"â€¢ {comp_name}:\n")
                    f.write(f"  - í‰ê· : {data['mean']:.3f}ì´ˆ ({data['percentage_of_target']:.1f}% of target)\n")
                    f.write(f"  - ìµœëŒ€: {data['max']:.3f}ì´ˆ\n")
                    if data['bottleneck_frequency'] > 0:
                        f.write(f"  - ë³‘ëª© ë¹ˆë„: {data['bottleneck_frequency']}íšŒ\n")
                    f.write("\n")
            
            # ë³‘ëª©ì  ë¶„ì„
            if bottlenecks:
                f.write("3. ì„±ëŠ¥ ë³‘ëª©ì  ë° ê¶Œì¥ì‚¬í•­\n")
                f.write("-"*30 + "\n")
                
                for i, bottleneck in enumerate(bottlenecks, 1):
                    f.write(f"{i}. {bottleneck['component']}\n")
                    f.write(f"   â€¢ í‰ê·  ì‹œê°„: {bottleneck['average_time']:.3f}ì´ˆ\n")
                    f.write(f"   â€¢ ëª©í‘œ ëŒ€ë¹„: {bottleneck['percentage_of_target']:.1f}%\n")
                    f.write(f"   â€¢ ì‹¬ê°ë„: {bottleneck['severity']}\n")
                    if 'frequency' in bottleneck:
                        f.write(f"   â€¢ ë°œìƒ ë¹ˆë„: {bottleneck['frequency']}íšŒ\n")
                    f.write(f"   â€¢ ê¶Œì¥ì‚¬í•­: {bottleneck['recommendation']}\n\n")
            
            # ì¢…í•© ê¶Œì¥ì‚¬í•­
            f.write("4. ì¢…í•© ê¶Œì¥ì‚¬í•­\n")
            f.write("-"*30 + "\n")
            
            if stats['target_achievement_rate'] >= 0.9:
                f.write("â€¢ ì „ì²´ì ìœ¼ë¡œ ëª©í‘œ ì„±ëŠ¥ì„ ë§Œì¡±í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n")
                f.write("â€¢ í˜„ì¬ ìµœì í™” ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”.\n")
            elif stats['target_achievement_rate'] >= 0.7:
                f.write("â€¢ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³  ìˆìœ¼ë‚˜ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n")
                f.write("â€¢ ì£¼ìš” ë³‘ëª©ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”.\n")
            else:
                f.write("â€¢ ëª©í‘œ ë‹¬ì„±ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹œê¸‰í•œ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n")
                f.write("â€¢ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì¬ê²€í† ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n")
            
            if stats['mean'] > self.target_time:
                f.write(f"â€¢ í‰ê·  ì‹¤í–‰ ì‹œê°„({stats['mean']:.3f}ì´ˆ)ì´ ëª©í‘œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\n")
                f.write("â€¢ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë˜ëŠ” í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n")
            
        logger.info(f"ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ê·¸ë˜í”„ë„ ìƒì„±
        self.generate_performance_plots(df)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="DEFIPOSER-ARB ì„±ëŠ¥ ë¶„ì„ ë„êµ¬"
    )
    parser.add_argument(
        "--log-file", 
        default="logs/performance_benchmark.json",
        help="ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output-dir",
        default="reports",
        help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--generate-plots",
        action="store_true",
        help="ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"
    )
    
    args = parser.parse_args()
    
    # ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = PerformanceAnalyzer(args.log_file)
    
    print("ğŸ” DEFIPOSER-ARB ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: í‰ê·  {analyzer.target_time}ì´ˆ ì´í•˜ ì‹¤í–‰")
    
    # ë³´ê³ ì„œ ìƒì„±
    report_file = Path(args.output_dir) / "performance_analysis_report.txt"
    analyzer.generate_report(str(report_file))
    
    # ì‹¤ì‹œê°„ ì„±ëŠ¥ ë³´ê³ ì„œ ì¶œë ¥
    try:
        current_report = get_performance_report(last_n_blocks=50)
        if "error" not in current_report:
            print(f"\nğŸ“ˆ ìµœê·¼ 50ë¸”ë¡ ì„±ëŠ¥:")
            summary = current_report["summary"]
            print(f"   ì„±ê³µë¥ : {summary['success_rate']:.1%}")
            print(f"   í‰ê·  ì‹œê°„: {summary['average_time']:.3f}ì´ˆ")
            print(f"   ìµœê³  ê¸°ë¡: {summary['fastest_time']:.3f}ì´ˆ")
        else:
            print(f"âš ï¸ ì‹¤ì‹œê°„ ë°ì´í„° ì—†ìŒ: {current_report['error']}")
    except Exception as e:
        print(f"âš ï¸ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ëŠ” {args.output_dir}/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()