#!/usr/bin/env python3
"""
Memory Leak and Performance Degradation Check
ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ì €í•˜ ì ê²€ ì‹œìŠ¤í…œ

DeFiPoser-ARB êµ¬í˜„ì—ì„œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ì™€ ì„±ëŠ¥ ì €í•˜ë¥¼ ê°ì§€í•˜ê³  ë¶„ì„
"""

import asyncio
import gc
import logging
import json
import time
import tracemalloc
import psutil
import resource
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import sqlite3
import threading
import weakref
from collections import deque
import sys
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass 
class MemorySnapshot:
    timestamp: datetime
    memory_usage_mb: float
    peak_memory_mb: float
    object_count: int
    garbage_count: int
    active_threads: int
    open_files: int
    cpu_percent: float
    top_memory_objects: List[Tuple[str, int]]

@dataclass
class PerformanceMetric:
    timestamp: datetime  
    operation_name: str
    execution_time: float
    memory_before: float
    memory_after: float
    memory_delta: float
    cpu_usage: float
    success: bool
    details: Dict

class MemoryLeakDetector:
    """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ê¸°"""
    
    def __init__(self, check_interval: int = 60):
        self.check_interval = check_interval
        self.snapshots: deque = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ìŠ¤ëƒ…ìƒ· ë³´ê´€
        self.performance_metrics: deque = deque(maxlen=1000)
        self.is_monitoring = False
        self.monitor_thread = None
        self.weak_references = set()
        
        # íŠ¸ë ˆì´ìŠ¤ ë©”ëª¨ë¦¬ ì‹œì‘
        tracemalloc.start()
        
        # ì´ˆê¸° í”„ë¡œì„¸ìŠ¤ ì •ë³´
        self.process = psutil.Process(os.getpid())
        self.initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            logger.warning("ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤.")
            return
            
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")
        
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
        
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_monitoring:
            try:
                snapshot = self._take_memory_snapshot()
                self.snapshots.append(snapshot)
                
                # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€
                if len(self.snapshots) >= 3:
                    self._check_memory_leak()
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(self.check_interval)
                
    def _take_memory_snapshot(self) -> MemorySnapshot:
        """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        try:
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = self.process.memory_info()
            current_memory = memory_info.rss / 1024 / 1024  # MB
            
            # Peak memory from tracemalloc
            current_trace, peak_trace = tracemalloc.get_traced_memory()
            peak_memory = peak_trace / 1024 / 1024  # MB
            
            # ê°ì²´ ìˆ˜ ë° ê°€ë¹„ì§€ ìˆ˜ì§‘
            gc.collect()  # ê°•ì œ ê°€ë¹„ì§€ ìˆ˜ì§‘
            object_count = len(gc.get_objects())
            garbage_count = len(gc.garbage)
            
            # ìŠ¤ë ˆë“œ ë° íŒŒì¼ í•¸ë“¤
            active_threads = threading.active_count()
            try:
                open_files = len(self.process.open_files())
            except (psutil.AccessDenied, psutil.NoSuchProcess):
                open_files = -1
                
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = self.process.cpu_percent()
            
            # ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê°ì²´ íƒ‘ 10
            top_objects = self._get_top_memory_objects()
            
            return MemorySnapshot(
                timestamp=datetime.now(),
                memory_usage_mb=current_memory,
                peak_memory_mb=peak_memory,
                object_count=object_count,
                garbage_count=garbage_count,
                active_threads=active_threads,
                open_files=open_files,
                cpu_percent=cpu_percent,
                top_memory_objects=top_objects
            )
            
        except Exception as e:
            logger.error(f"ìŠ¤ëƒ…ìƒ· ìƒì„± ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ìŠ¤ëƒ…ìƒ· ë°˜í™˜
            return MemorySnapshot(
                timestamp=datetime.now(),
                memory_usage_mb=0,
                peak_memory_mb=0,
                object_count=0,
                garbage_count=0,
                active_threads=0,
                open_files=0,
                cpu_percent=0,
                top_memory_objects=[]
            )
    
    def _get_top_memory_objects(self) -> List[Tuple[str, int]]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë“¤ ë°˜í™˜"""
        try:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            
            result = []
            for stat in top_stats[:10]:
                result.append((str(stat.traceback), stat.size))
                
            return result
            
        except Exception as e:
            logger.error(f"í†± ë©”ëª¨ë¦¬ ê°ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _check_memory_leak(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì²´í¬"""
        if len(self.snapshots) < 3:
            return
            
        # ìµœê·¼ 3ê°œ ìŠ¤ëƒ…ìƒ· ë¶„ì„
        recent_snapshots = list(self.snapshots)[-3:]
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ì¶”ì„¸ ë¶„ì„
        memory_values = [s.memory_usage_mb for s in recent_snapshots]
        
        # ì—°ì†ì ì¸ ì¦ê°€ì¸ì§€ í™•ì¸
        is_increasing = all(memory_values[i] <= memory_values[i+1] for i in range(len(memory_values)-1))
        
        if is_increasing:
            increase_rate = (memory_values[-1] - memory_values[0]) / len(memory_values)
            
            # ì¦ê°€ìœ¨ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ê²½ê³ 
            if increase_rate > 5:  # 5MB/snapshot ì´ìƒ ì¦ê°€
                logger.warning(f"âš ï¸  ì ì¬ì  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€: {increase_rate:.2f} MB/snapshot ì¦ê°€")
                logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_values[0]:.2f} -> {memory_values[-1]:.2f} MB")
                
                # ìƒì„¸ ë¶„ì„
                self._analyze_memory_leak(recent_snapshots)
    
    def _analyze_memory_leak(self, snapshots: List[MemorySnapshot]):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìƒì„¸ ë¶„ì„"""
        logger.info("ğŸ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìƒì„¸ ë¶„ì„ ì¤‘...")
        
        first_snapshot = snapshots[0]
        last_snapshot = snapshots[-1]
        
        # ê°ì²´ ìˆ˜ ë³€í™”
        object_increase = last_snapshot.object_count - first_snapshot.object_count
        logger.info(f"ê°ì²´ ìˆ˜ ë³€í™”: {object_increase}")
        
        # ê°€ë¹„ì§€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ê°ì²´
        if last_snapshot.garbage_count > 0:
            logger.warning(f"ê°€ë¹„ì§€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ê°ì²´: {last_snapshot.garbage_count}")
            
        # ìŠ¤ë ˆë“œ ë° íŒŒì¼ í•¸ë“¤ ë³€í™”
        thread_increase = last_snapshot.active_threads - first_snapshot.active_threads
        if thread_increase > 0:
            logger.warning(f"í™œì„± ìŠ¤ë ˆë“œ ì¦ê°€: +{thread_increase}")
            
        if last_snapshot.open_files > 0 and first_snapshot.open_files > 0:
            file_increase = last_snapshot.open_files - first_snapshot.open_files
            if file_increase > 0:
                logger.warning(f"ì—´ë¦° íŒŒì¼ í•¸ë“¤ ì¦ê°€: +{file_increase}")

    def measure_performance(self, operation_name: str):
        """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬"""
        def decorator(func):
            async def async_wrapper(*args, **kwargs):
                return await self._measure_async_performance(operation_name, func, *args, **kwargs)
                
            def sync_wrapper(*args, **kwargs):
                return self._measure_sync_performance(operation_name, func, *args, **kwargs)
                
            if asyncio.iscoroutinefunction(func):
                return async_wrapper
            else:
                return sync_wrapper
                
        return decorator
    
    async def _measure_async_performance(self, operation_name: str, func, *args, **kwargs):
        """ë¹„ë™ê¸° í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •"""
        # ì‹œì‘ ì „ ìƒíƒœ
        start_time = time.time()
        memory_before = self.process.memory_info().rss / 1024 / 1024
        cpu_before = self.process.cpu_percent()
        
        success = False
        result = None
        error = None
        
        try:
            result = await func(*args, **kwargs)
            success = True
        except Exception as e:
            error = str(e)
            logger.error(f"ì„±ëŠ¥ ì¸¡ì • ì¤‘ ì˜¤ë¥˜ ({operation_name}): {e}")
        
        # ì¢…ë£Œ í›„ ìƒíƒœ
        end_time = time.time()
        memory_after = self.process.memory_info().rss / 1024 / 1024
        execution_time = end_time - start_time
        memory_delta = memory_after - memory_before
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        metric = PerformanceMetric(
            timestamp=datetime.now(),
            operation_name=operation_name,
            execution_time=execution_time,
            memory_before=memory_before,
            memory_after=memory_after,
            memory_delta=memory_delta,
            cpu_usage=self.process.cpu_percent() - cpu_before,
            success=success,
            details={
                'args_count': len(args),
                'kwargs_count': len(kwargs),
                'error': error
            }
        )
        
        self.performance_metrics.append(metric)
        
        # ì„±ëŠ¥ ì €í•˜ ì²´í¬
        if execution_time > 10:  # 10ì´ˆ ì´ìƒ
            logger.warning(f"âš ï¸  ì„±ëŠ¥ ì €í•˜ ê°ì§€ ({operation_name}): {execution_time:.2f}ì´ˆ")
            
        if memory_delta > 50:  # 50MB ì´ìƒ ì¦ê°€
            logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ({operation_name}): +{memory_delta:.2f}MB")
        
        if error:
            raise Exception(error)
            
        return result
    
    def _measure_sync_performance(self, operation_name: str, func, *args, **kwargs):
        """ë™ê¸° í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •"""
        # ì‹œì‘ ì „ ìƒíƒœ
        start_time = time.time()
        memory_before = self.process.memory_info().rss / 1024 / 1024
        cpu_before = self.process.cpu_percent()
        
        success = False
        result = None
        error = None
        
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            error = str(e)
            logger.error(f"ì„±ëŠ¥ ì¸¡ì • ì¤‘ ì˜¤ë¥˜ ({operation_name}): {e}")
        
        # ì¢…ë£Œ í›„ ìƒíƒœ
        end_time = time.time()
        memory_after = self.process.memory_info().rss / 1024 / 1024
        execution_time = end_time - start_time
        memory_delta = memory_after - memory_before
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        metric = PerformanceMetric(
            timestamp=datetime.now(),
            operation_name=operation_name,
            execution_time=execution_time,
            memory_before=memory_before,
            memory_after=memory_after,
            memory_delta=memory_delta,
            cpu_usage=self.process.cpu_percent() - cpu_before,
            success=success,
            details={
                'args_count': len(args),
                'kwargs_count': len(kwargs),
                'error': error
            }
        )
        
        self.performance_metrics.append(metric)
        
        # ì„±ëŠ¥ ì €í•˜ ì²´í¬
        if execution_time > 10:  # 10ì´ˆ ì´ìƒ
            logger.warning(f"âš ï¸  ì„±ëŠ¥ ì €í•˜ ê°ì§€ ({operation_name}): {execution_time:.2f}ì´ˆ")
            
        if memory_delta > 50:  # 50MB ì´ìƒ ì¦ê°€
            logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ({operation_name}): +{memory_delta:.2f}MB")
        
        if error:
            raise Exception(error)
            
        return result
    
    def generate_report(self) -> Dict:
        """ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        now = datetime.now()
        
        # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë¶„ì„
        memory_analysis = self._analyze_memory_snapshots()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
        performance_analysis = self._analyze_performance_metrics()
        
        # ì‹œìŠ¤í…œ ì •ë³´
        system_info = {
            'python_version': sys.version,
            'process_id': os.getpid(),
            'initial_memory_mb': self.initial_memory,
            'current_memory_mb': self.process.memory_info().rss / 1024 / 1024,
            'memory_growth_mb': (self.process.memory_info().rss / 1024 / 1024) - self.initial_memory,
            'monitoring_duration_minutes': len(self.snapshots) * (self.check_interval / 60),
            'total_snapshots': len(self.snapshots),
            'total_performance_metrics': len(self.performance_metrics)
        }
        
        report = {
            'report_metadata': {
                'generated_at': now.isoformat(),
                'report_type': 'memory_leak_performance_check',
                'monitoring_active': self.is_monitoring
            },
            'system_info': system_info,
            'memory_analysis': memory_analysis,
            'performance_analysis': performance_analysis,
            'recommendations': self._generate_recommendations(memory_analysis, performance_analysis)
        }
        
        return report
    
    def _analyze_memory_snapshots(self) -> Dict:
        """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë¶„ì„"""
        if not self.snapshots:
            return {'error': 'No memory snapshots available'}
            
        snapshots_list = list(self.snapshots)
        
        # ê¸°ë³¸ í†µê³„
        memory_values = [s.memory_usage_mb for s in snapshots_list]
        object_counts = [s.object_count for s in snapshots_list]
        thread_counts = [s.active_threads for s in snapshots_list]
        
        analysis = {
            'memory_usage': {
                'min_mb': min(memory_values),
                'max_mb': max(memory_values),
                'avg_mb': sum(memory_values) / len(memory_values),
                'current_mb': memory_values[-1],
                'trend': self._calculate_trend(memory_values)
            },
            'object_count': {
                'min': min(object_counts),
                'max': max(object_counts),
                'avg': sum(object_counts) / len(object_counts),
                'current': object_counts[-1],
                'trend': self._calculate_trend(object_counts)
            },
            'thread_count': {
                'min': min(thread_counts),
                'max': max(thread_counts),
                'avg': sum(thread_counts) / len(thread_counts),
                'current': thread_counts[-1]
            },
            'garbage_collection': {
                'total_garbage': sum(s.garbage_count for s in snapshots_list),
                'max_garbage': max(s.garbage_count for s in snapshots_list),
                'avg_garbage': sum(s.garbage_count for s in snapshots_list) / len(snapshots_list)
            }
        }
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜ë„ í‰ê°€
        analysis['leak_risk'] = self._assess_leak_risk(analysis)
        
        return analysis
    
    def _analyze_performance_metrics(self) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„"""
        if not self.performance_metrics:
            return {'error': 'No performance metrics available'}
            
        metrics_list = list(self.performance_metrics)
        
        # ì—°ì‚°ë³„ ê·¸ë£¹í™”
        operations = {}
        for metric in metrics_list:
            op_name = metric.operation_name
            if op_name not in operations:
                operations[op_name] = []
            operations[op_name].append(metric)
        
        # ê° ì—°ì‚°ë³„ ë¶„ì„
        operation_analysis = {}
        for op_name, op_metrics in operations.items():
            exec_times = [m.execution_time for m in op_metrics]
            memory_deltas = [m.memory_delta for m in op_metrics]
            success_rate = len([m for m in op_metrics if m.success]) / len(op_metrics)
            
            operation_analysis[op_name] = {
                'total_calls': len(op_metrics),
                'success_rate': success_rate,
                'execution_time': {
                    'min_seconds': min(exec_times),
                    'max_seconds': max(exec_times),
                    'avg_seconds': sum(exec_times) / len(exec_times),
                    'trend': self._calculate_trend(exec_times)
                },
                'memory_impact': {
                    'min_delta_mb': min(memory_deltas),
                    'max_delta_mb': max(memory_deltas),
                    'avg_delta_mb': sum(memory_deltas) / len(memory_deltas)
                }
            }
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        all_exec_times = [m.execution_time for m in metrics_list]
        all_memory_deltas = [m.memory_delta for m in metrics_list]
        overall_success_rate = len([m for m in metrics_list if m.success]) / len(metrics_list)
        
        analysis = {
            'overall_metrics': {
                'total_operations': len(metrics_list),
                'unique_operations': len(operations),
                'overall_success_rate': overall_success_rate,
                'avg_execution_time': sum(all_exec_times) / len(all_exec_times),
                'avg_memory_impact': sum(all_memory_deltas) / len(all_memory_deltas)
            },
            'operation_breakdown': operation_analysis,
            'performance_issues': self._identify_performance_issues(operation_analysis)
        }
        
        return analysis
    
    def _calculate_trend(self, values: List[float]) -> str:
        """ê°’ë“¤ì˜ ì¶”ì„¸ ê³„ì‚°"""
        if len(values) < 2:
            return 'insufficient_data'
            
        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        n = len(values)
        x = list(range(n))
        
        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        if (n * sum_x2 - sum_x ** 2) == 0:
            return 'stable'
            
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        
        if slope > 0.1:
            return 'increasing'
        elif slope < -0.1:
            return 'decreasing'
        else:
            return 'stable'
    
    def _assess_leak_risk(self, memory_analysis: Dict) -> Dict:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜ë„ í‰ê°€"""
        risk_score = 0
        risk_factors = []
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì„¸
        if memory_analysis['memory_usage']['trend'] == 'increasing':
            risk_score += 3
            risk_factors.append('Memory usage is consistently increasing')
        
        # ê°ì²´ ìˆ˜ ì¶”ì„¸
        if memory_analysis['object_count']['trend'] == 'increasing':
            risk_score += 2
            risk_factors.append('Object count is consistently increasing')
        
        # ê°€ë¹„ì§€ ìˆ˜ì§‘ íš¨ìœ¨ì„±
        if memory_analysis['garbage_collection']['avg_garbage'] > 100:
            risk_score += 2
            risk_factors.append('High number of uncollected garbage objects')
        
        # ë©”ëª¨ë¦¬ ì„±ì¥ë¥ 
        current_memory = memory_analysis['memory_usage']['current_mb']
        min_memory = memory_analysis['memory_usage']['min_mb']
        growth_ratio = (current_memory - min_memory) / min_memory if min_memory > 0 else 0
        
        if growth_ratio > 0.5:  # 50% ì´ìƒ ì¦ê°€
            risk_score += 3
            risk_factors.append(f'Memory usage increased by {growth_ratio:.1%}')
        
        # ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •
        if risk_score >= 7:
            risk_level = 'HIGH'
        elif risk_score >= 4:
            risk_level = 'MEDIUM'
        elif risk_score >= 1:
            risk_level = 'LOW'
        else:
            risk_level = 'MINIMAL'
        
        return {
            'risk_level': risk_level,
            'risk_score': risk_score,
            'risk_factors': risk_factors
        }
    
    def _identify_performance_issues(self, operation_analysis: Dict) -> List[str]:
        """ì„±ëŠ¥ ë¬¸ì œ ì‹ë³„"""
        issues = []
        
        for op_name, analysis in operation_analysis.items():
            # ì‹¤í–‰ ì‹œê°„ ë¬¸ì œ
            avg_time = analysis['execution_time']['avg_seconds']
            if avg_time > 6.43:  # ë…¼ë¬¸ì˜ ëª©í‘œ ì‹œê°„
                issues.append(f'{op_name}: Average execution time ({avg_time:.2f}s) exceeds target (6.43s)')
            
            # ì„±ê³µë¥  ë¬¸ì œ
            success_rate = analysis['success_rate']
            if success_rate < 0.9:  # 90% ë¯¸ë§Œ
                issues.append(f'{op_name}: Low success rate ({success_rate:.1%})')
            
            # ë©”ëª¨ë¦¬ ì¦ê°€ ë¬¸ì œ
            avg_memory_delta = analysis['memory_impact']['avg_delta_mb']
            if avg_memory_delta > 10:  # 10MB ì´ìƒ ì¦ê°€
                issues.append(f'{op_name}: High memory usage per operation (+{avg_memory_delta:.1f}MB)')
            
            # ì‹¤í–‰ ì‹œê°„ ì¦ê°€ ì¶”ì„¸
            if analysis['execution_time']['trend'] == 'increasing':
                issues.append(f'{op_name}: Execution time is increasing over time')
        
        return issues
    
    def _generate_recommendations(self, memory_analysis: Dict, performance_analysis: Dict) -> List[str]:
        """ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë©”ëª¨ë¦¬ ê´€ë ¨ ê¶Œê³ 
        if 'memory_analysis' in memory_analysis:
            risk_level = memory_analysis.get('leak_risk', {}).get('risk_level', 'UNKNOWN')
            
            if risk_level in ['HIGH', 'MEDIUM']:
                recommendations.append('Consider implementing more aggressive garbage collection')
                recommendations.append('Review object lifecycle management and ensure proper cleanup')
                recommendations.append('Use weak references for cached data that can be recreated')
        
        # ì„±ëŠ¥ ê´€ë ¨ ê¶Œê³ 
        if 'performance_issues' in performance_analysis:
            issues = performance_analysis['performance_issues']
            
            if any('execution time' in issue.lower() for issue in issues):
                recommendations.append('Optimize slow operations with caching or parallel processing')
                recommendations.append('Consider breaking down complex operations into smaller chunks')
            
            if any('memory usage' in issue.lower() for issue in issues):
                recommendations.append('Implement memory pooling for frequently allocated objects')
                recommendations.append('Use streaming processing for large datasets')
            
            if any('success rate' in issue.lower() for issue in issues):
                recommendations.append('Improve error handling and retry mechanisms')
                recommendations.append('Add input validation to prevent operation failures')
        
        # ì¼ë°˜ì ì¸ ê¶Œê³ ì‚¬í•­
        if not recommendations:
            recommendations.append('System is performing well, continue monitoring')
        else:
            recommendations.append('Consider running this check more frequently during high-load periods')
        
        return recommendations

class DeFiSystemMemoryCheck:
    """DeFi ì‹œìŠ¤í…œ ì „ìš© ë©”ëª¨ë¦¬ ì ê²€"""
    
    def __init__(self):
        self.detector = MemoryLeakDetector(check_interval=30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        
    async def run_comprehensive_check(self) -> str:
        """ì¢…í•©ì ì¸ ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ì ê²€"""
        logger.info("ğŸ” DeFi ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ì ê²€ ì‹œì‘")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.detector.start_monitoring()
        
        try:
            # ì£¼ìš” DeFi ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ í…ŒìŠ¤íŠ¸
            await self._simulate_defi_operations()
            
            # ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë©”ëª¨ë¦¬ íŒ¨í„´ ê´€ì°°
            logger.info("â³ ë©”ëª¨ë¦¬ íŒ¨í„´ ê´€ì°°ì„ ìœ„í•´ ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(180)  # 3ë¶„ ëŒ€ê¸°
            
            # ë³´ê³ ì„œ ìƒì„±
            report = self.detector.generate_report()
            
            # ë³´ê³ ì„œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"memory_leak_check_report_{timestamp}.json"
            
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            # ìš”ì•½ ì¶œë ¥
            self._print_summary(report)
            
            return report_filename
            
        finally:
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.detector.stop_monitoring()
    
    async def _simulate_defi_operations(self):
        """DeFi ì‘ì—…ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´ í™•ì¸"""
        
        # 1. ë°ì´í„° ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜
        await self._simulate_data_collection()
        
        # 2. ì•„ë¹„íŠ¸ë˜ì§€ íƒì§€ ì‹œë®¬ë ˆì´ì…˜
        await self._simulate_arbitrage_detection()
        
        # 3. ê·¸ë˜í”„ ë¹Œë”© ì‹œë®¬ë ˆì´ì…˜
        await self._simulate_graph_building()
        
        # 4. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        await self._simulate_memory_intensive_operations()
    
    @property 
    def _measured_data_collection(self):
        return self.detector.measure_performance("data_collection")
    
    @property
    def _measured_arbitrage_detection(self):
        return self.detector.measure_performance("arbitrage_detection")
    
    @property
    def _measured_graph_building(self):
        return self.detector.measure_performance("graph_building")
    
    @property
    def _measured_memory_intensive_op(self):
        return self.detector.measure_performance("memory_intensive_operation")
    
    async def _simulate_data_collection(self):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜"""
        @self._measured_data_collection
        async def collect_data():
            # ëŒ€ëŸ‰ì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œë®¬ë ˆì´ì…˜
            data = []
            for i in range(10000):
                data.append({
                    'block_number': i,
                    'price_data': [random.uniform(1, 1000) for _ in range(25)],  # 25ê°œ ìì‚°
                    'pool_data': {f'pool_{j}': {'reserve0': random.uniform(1000, 100000), 
                                              'reserve1': random.uniform(1000, 100000)} 
                                for j in range(96)}  # 96ê°œ í”„ë¡œí† ì½œ
                })
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì¼ë¶€ ë°ì´í„° ë³´ê´€
            return data[:1000]  # ì¼ë¶€ë§Œ ë°˜í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ í™•ì¸
        
        import random
        result = await collect_data()
        logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ í•­ëª©")
    
    async def _simulate_arbitrage_detection(self):
        """ì•„ë¹„íŠ¸ë˜ì§€ íƒì§€ ì‹œë®¬ë ˆì´ì…˜"""
        @self._measured_arbitrage_detection
        async def detect_arbitrage():
            opportunities = []
            
            # Bellman-Ford ì•Œê³ ë¦¬ì¦˜ ì‹œë®¬ë ˆì´ì…˜
            for cycle in range(100):  # 100ê°œì˜ negative cycle ì²´í¬
                path = []
                for step in range(5):  # í‰ê·  5ë‹¨ê³„ ê²½ë¡œ
                    path.append({
                        'dex': f'dex_{step}',
                        'asset_in': f'asset_{step}',
                        'asset_out': f'asset_{(step+1)%25}',
                        'exchange_rate': random.uniform(0.95, 1.05)
                    })
                
                # ìˆ˜ìµì„± ê³„ì‚°
                profit = random.uniform(-0.1, 0.5)
                if profit > 0:
                    opportunities.append({
                        'path': path,
                        'profit': profit,
                        'confidence': random.uniform(0.7, 0.95)
                    })
            
            return opportunities
        
        import random
        result = await detect_arbitrage()
        logger.info(f"ì•„ë¹„íŠ¸ë˜ì§€ íƒì§€ ì™„ë£Œ: {len(result)}ê°œ ê¸°íšŒ ë°œê²¬")
    
    async def _simulate_graph_building(self):
        """ê·¸ë˜í”„ ë¹Œë”© ì‹œë®¬ë ˆì´ì…˜"""
        @self._measured_graph_building
        async def build_graph():
            # 25ê°œ ë…¸ë“œ, 2400ê°œ ì—£ì§€ (96 protocols * 25 assets) ê·¸ë˜í”„ êµ¬ì¶•
            nodes = [f'asset_{i}' for i in range(25)]
            edges = {}
            
            for i in range(25):
                for j in range(25):
                    if i != j:
                        edge_key = f"{nodes[i]}_{nodes[j]}"
                        edges[edge_key] = {
                            'weight': random.uniform(-0.1, 0.1),  # log price
                            'protocols': [f'protocol_{k}' for k in range(random.randint(1, 5))]
                        }
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê·¸ë˜í”„ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
            adjacency_matrix = [[0.0 for _ in range(25)] for _ in range(25)]
            for i in range(25):
                for j in range(25):
                    adjacency_matrix[i][j] = random.uniform(-0.5, 0.5)
            
            return {'nodes': nodes, 'edges': edges, 'matrix': adjacency_matrix}
        
        import random
        result = await build_graph()
        logger.info(f"ê·¸ë˜í”„ ë¹Œë”© ì™„ë£Œ: {len(result['nodes'])}ê°œ ë…¸ë“œ, {len(result['edges'])}ê°œ ì—£ì§€")
    
    async def _simulate_memory_intensive_operations(self):
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
        @self._measured_memory_intensive_op
        async def intensive_operation():
            # í° ë°ì´í„° êµ¬ì¡° ìƒì„± ë° ì¡°ì‘
            large_data = []
            
            # 96ê°œ í”„ë¡œí† ì½œ * 25ê°œ ìì‚° * 150ì¼ = 360,000 ë°ì´í„° í¬ì¸íŠ¸
            for protocol in range(96):
                for asset in range(25):
                    for day in range(150):
                        large_data.append({
                            'protocol': f'protocol_{protocol}',
                            'asset': f'asset_{asset}',
                            'day': day,
                            'transactions': [
                                {
                                    'timestamp': datetime.now() - timedelta(days=day, hours=hour),
                                    'amount': random.uniform(0.1, 100),
                                    'price': random.uniform(1, 1000)
                                }
                                for hour in range(24)  # ì‹œê°„ë‹¹ ë°ì´í„°
                            ]
                        })
            
            # ë°ì´í„° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€)
            processed_data = {}
            for item in large_data:
                key = f"{item['protocol']}_{item['asset']}"
                if key not in processed_data:
                    processed_data[key] = []
                processed_data[key].append(item)
            
            # ì¼ë¶€ ë°ì´í„°ë§Œ ë°˜í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ í…ŒìŠ¤íŠ¸
            return {k: v for k, v in list(processed_data.items())[:100]}
        
        import random
        result = await intensive_operation()
        logger.info(f"ì§‘ì•½ì  ì—°ì‚° ì™„ë£Œ: {len(result)}ê°œ ì²˜ë¦¬ëœ í‚¤")
    
    def _print_summary(self, report: Dict):
        """ë³´ê³ ì„œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ” DeFi ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ì ê²€ ê²°ê³¼")
        print("="*70)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        system_info = report.get('system_info', {})
        print(f"\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"  ì´ˆê¸° ë©”ëª¨ë¦¬: {system_info.get('initial_memory_mb', 0):.2f} MB")
        print(f"  í˜„ì¬ ë©”ëª¨ë¦¬: {system_info.get('current_memory_mb', 0):.2f} MB")
        print(f"  ë©”ëª¨ë¦¬ ì¦ê°€: {system_info.get('memory_growth_mb', 0):.2f} MB")
        print(f"  ëª¨ë‹ˆí„°ë§ ì‹œê°„: {system_info.get('monitoring_duration_minutes', 0):.1f} ë¶„")
        
        # ë©”ëª¨ë¦¬ ë¶„ì„
        memory_analysis = report.get('memory_analysis', {})
        if 'leak_risk' in memory_analysis:
            leak_risk = memory_analysis['leak_risk']
            print(f"\nâš ï¸  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜ë„: {leak_risk.get('risk_level', 'UNKNOWN')}")
            print(f"  ìœ„í—˜ ì ìˆ˜: {leak_risk.get('risk_score', 0)}/10")
            
            risk_factors = leak_risk.get('risk_factors', [])
            if risk_factors:
                print(f"  ìœ„í—˜ ìš”ì†Œ:")
                for factor in risk_factors:
                    print(f"    â€¢ {factor}")
        
        # ì„±ëŠ¥ ë¶„ì„
        performance_analysis = report.get('performance_analysis', {})
        if 'overall_metrics' in performance_analysis:
            overall = performance_analysis['overall_metrics']
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"  ì „ì²´ ì—°ì‚°: {overall.get('total_operations', 0)}ê°œ")
            print(f"  ì„±ê³µë¥ : {overall.get('overall_success_rate', 0):.1%}")
            print(f"  í‰ê·  ì‹¤í–‰ì‹œê°„: {overall.get('avg_execution_time', 0):.2f}ì´ˆ")
            print(f"  í‰ê·  ë©”ëª¨ë¦¬ ì˜í–¥: {overall.get('avg_memory_impact', 0):.2f}MB")
        
        # ì„±ëŠ¥ ë¬¸ì œ
        performance_issues = performance_analysis.get('performance_issues', [])
        if performance_issues:
            print(f"\nâŒ ì„±ëŠ¥ ë¬¸ì œ:")
            for issue in performance_issues:
                print(f"    â€¢ {issue}")
        
        # ê¶Œê³ ì‚¬í•­
        recommendations = report.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
            for rec in recommendations:
                print(f"    â€¢ {rec}")
        
        print("\n" + "="*70)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” DeFi ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ì €í•˜ ì ê²€")
    print("=" * 70)
    
    checker = DeFiSystemMemoryCheck()
    
    try:
        report_file = await checker.run_comprehensive_check()
        print(f"\nâœ… ì ê²€ ì™„ë£Œ! ìƒì„¸ ë³´ê³ ì„œ: {report_file}")
        
    except Exception as e:
        logger.error(f"ì ê²€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì ê²€ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(main())